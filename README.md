# Diverse Image Annotation (CVPR 2017)
 
This project introduces our work "Diverse Image Annotation" published at CVPR 2017. 

We propose a new task called diverse image annotation (DIA) to redefine the current automatic image annotation (AIA), to encourage the results of AIA to be more close to human annotations. 
The goal of most existing AIA models is to predict most relevant tags of the image. 
However, we have observed that there is a obvious gap between the results of AIA and the ones of human annotations. 
What is reason behind? Let's start with an observation of how human annotates images. 

The goal of diverse image annotation (DIA) is to as much useful information of an
image as possible using a limited number of tags. 
It requires the tags to be not only representative to
the image, but also diverse from each other to reduce redundancy. 
Why DIA is more reasonable

What is Diverse Image Annotation
----
We wish to redefine the current automatic image annotation (AIA), to encourage the results of AIA to be more 
close to human annotations. 
The goal of diverse image annotation (DIA) is to as much useful information of an
image as possible using a limited number of tags. 
It requires the tags to be not only representative to
the image, but also diverse from each other to reduce redundancy.


Human Annotations
----
Some results and statistics
![](https://github.com/wubaoyuan/DIA/tree/master/figures/human_annotation_toy_example.png)
Our model
----

##Semantic Paths


##Sampling





